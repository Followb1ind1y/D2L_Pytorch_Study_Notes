{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPDdfpINYiZTf+W6A1K5uY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Followb1ind1y/D2L_Pytorch_Study_Notes/blob/main/06_D2L_Builders_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dive into Deep Learning 中文学习笔记** \n",
        "# **6. 深度学习计算 （Builders’ Guide）**"
      ],
      "metadata": {
        "id": "7vb1fIbZbRWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.1. 模型构造（Layers and Modules）**\n",
        "\n",
        "为了实现一些复杂的网络，我们引入了神经网络 **模块（*Module*）** 的概念。一个模块可以描述一个单一的层，一个由多个层组成的组件，或者整个模型本身。使用模块抽象的一个好处是，它们可以被组合成更大的网络，并且通常是递归的。\n",
        "\n",
        "从编程的角度来看，一个模块由一个 **类（*Class*）** 来代表。它的任何子类必须定义一个前向传播方法，将其输入转化为输出，并且必须存储任何必要的参数。请注意，有些模块根本不需要任何参数。最后，一个模块必须拥有一个反向传播方法，用于计算梯度。\n",
        "\n",
        "我们重温一下之前用来实现 MLPs 的代码。下面的代码生成了一个具有 256 个单元的全连接隐藏层和 ReLU 激活的网络，然后是一个具有 10个 单元的全连接输出层（没有激活函数）。"
      ],
      "metadata": {
        "id": "_X0rJDNobyLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
        "\n",
        "X = torch.rand(2, 20)\n",
        "net(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXzcAvlFbe6l",
        "outputId": "d8f8781e-405f-4b41-9a76-c026c6d86357"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "在这个例子中，我们通过实例化 `nn.Sequential` 来构建我们的模型，并将各层的执行顺序作为参数传递。简而言之，`nn.Sequential` 定义了一种特殊的 `Module`，即在 PyTorch 中展示模块的类。它维护着一个有序的组成模块的列表。请注意，两个完全连接的层中的每一个都是 `Linear` Class 的实例，它本身就是 `Module` 的子类。前向传播（forward）方法也非常简单：它将列表中的每个模块连在一起，将每个模块的输出作为输入传给下一个模块。"
      ],
      "metadata": {
        "id": "kwXy3dbTjpa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.1.1. 自定义 Module（A Custom Module）**\n",
        "\n",
        "在我们实现自己的自定义模块之前，我们简要地总结一下每个模块必须提供的基本功能：\n",
        "\n",
        "1. 取得输入数据作为其前向传播方法的参数。\n",
        "\n",
        "2. 通过让前向传播方法返回一个值来生成一个输出。请注意，输出可能有与输入不同的形状。\n",
        "\n",
        "3. 计算其输出相对于输入的梯度，这可以通过其反向传播方法获得。\n",
        "\n",
        "4. 存储并提供对那些执行前向传播计算所需的参数。\n",
        "\n",
        "5. 根据需要初始化模型参数。"
      ],
      "metadata": {
        "id": "6BmjZTe8lBHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        # Call the constructor of the parent class nn.Module to perform\n",
        "        # the necessary initialization\n",
        "        super().__init__()\n",
        "        self.hidden = nn.LazyLinear(256)\n",
        "        self.out = nn.LazyLinear(10)\n",
        "\n",
        "    # Define the forward propagation of the model, that is, how to return the\n",
        "    # required model output based on the input X\n",
        "    def forward(self, X):\n",
        "        return self.out(F.relu(self.hidden(X)))"
      ],
      "metadata": {
        "id": "w2rmybmkjmwI"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Module` 类是 `nn` 模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 `Module` 类构造本节开头提到的多层感知机。这里定义的 MLP 类重载了 `Module` 类的 `__init__` 函数和 `forward` 函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。\n",
        "\n",
        "以上的 MLP 类中无须定义反向传播函数。系统将通过**自动求梯度**而自动生成反向传播所需的 `backward` 函数。\n",
        "\n",
        "我们可以实例化 MLP 类得到模型变量 `net`。下面的代码初始化 `net` 并传入输入数据 `X` 做一次前向计算。其中，`net(X)` 会调用 MLP 继承自 `Module` 类的 `__call__` 函数，这个函数将调用 MLP 类定义的 `forward` 函数来完成前向计算。"
      ],
      "metadata": {
        "id": "_Sb5ZqwOm_tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = MLP()\n",
        "print(net(X).shape)\n",
        "print(net(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbu4QaBtnAAI",
        "outputId": "8ee6b72b-6178-4408-a229-c7a662f16fef"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10])\n",
            "tensor([[ 0.2507,  0.0092,  0.0856,  0.1758,  0.3869, -0.1573,  0.1859,  0.1325,\n",
            "          0.0644, -0.0105],\n",
            "        [ 0.1898, -0.0754,  0.0787, -0.0019,  0.3339, -0.0243,  0.2162,  0.2272,\n",
            "          0.0414, -0.0550]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意，这里并没有将 `Module` 类命名为 `Layer`（层）或者 `Model`（模型）之类的名字，这是因为该类是一个可供自由组建的部件。它的子类既可以是一个层（如 PyTorch 提供的 `Linear` 类），又可以是一个模型（如这里定义的 MLP 类），或者是模型的一个部分。"
      ],
      "metadata": {
        "id": "hd1ROLtZoYEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.1.2. Sequential 类（The Sequential Module）**\n",
        "\n",
        "当模型的前向计算为简单串联各个层的计算时，`Sequential` 类可以通过**更加简单**的方式定义模型。这正是 `Sequential` 类的目的：它可以接收一个子模块的有序字典（`OrderedDict`）或者一系列子模块作为参数来逐一添加 `Module` 的实例，而模型的前向计算就是将这些实例**按添加的顺序逐一计算**。\n",
        "\n",
        "为了建立我们自己的简化 `MySequential`，我们只需要定义两个关键方法：1. 一个将模块逐一追加到列表中的方法。2. 一个前向传播的方法，将一个输入通过模块链，以它们被附加的相同顺序传递。下面的 `MySequential` 类提供了与默认 `Sequential` 类相同的功能。"
      ],
      "metadata": {
        "id": "EuXfSUQuo0bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MySequential(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        for idx, module in enumerate(args):\n",
        "            self.add_module(str(idx), module)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for module in self.children():\n",
        "            X = module(X)\n",
        "        return X"
      ],
      "metadata": {
        "id": "UYxm8-9RnbbX"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 `__init__` 方法中，我们通过调用 `add_modules` 方法添加每个模块。这些模块可以在以后被 children 方法访问。通过这种方式，系统知道所添加的模块，它将正确地初始化每个模块的参数。\n",
        "\n",
        "当我们的 `MySequential` 的前向传播方法被调用时，每个添加的模块都会按照它们被添加的顺序执行。现在我们可以使用我们的 `MySequential` 类重新实现一个 MLP。"
      ],
      "metadata": {
        "id": "LGxapw8npwHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
        "net(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZS5vppFp5Oq",
        "outputId": "ead1becc-febd-479c-eaaf-54991505992a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.1.3. 在前向传播法中执行代码（Executing Code in the Forward Propagation Method）**\n",
        "\n",
        "虽然上面介绍的这些类可以使模型构造更加简单，且不需要定义 `forward` 函数，但直接继承 `Module` 类可以极大地拓展模型构造的灵活性。然而，有时我们可能想加入一些既不是前几层的结果也不是可更新参数的部分。我们称这些为 **恒定参数 (*constant parameters*)**。例如，我们想要一个计算函数 $f(x,w)=c \\cdot w^{T}x$ 的层，其中 $x$ 输入，$w$ 是我们的参数，并且 $c$ 是一些指定的常数，在优化过程中不被更新。所以我们实现一个 `FixedHiddenMLP` 类，如下所示:"
      ],
      "metadata": {
        "id": "0UDhstcM0L4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedHiddenMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Random weight parameters that will not compute gradients and\n",
        "        # therefore keep constant during training\n",
        "        self.rand_weight = torch.rand((20, 20), requires_grad=False) # 不可训练参数（常数参数）\n",
        "        self.linear = nn.LazyLinear(20)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.linear(X)\n",
        "        X = F.relu(X @ self.rand_weight + 1)\n",
        "        # Reuse the fully connected layer. This is equivalent to sharing\n",
        "        # parameters with two fully connected layers\n",
        "        X = self.linear(X)\n",
        "        # Control flow\n",
        "        while X.abs().sum() > 1:\n",
        "            X /= 2\n",
        "        return X.sum()"
      ],
      "metadata": {
        "id": "3YdtiCNUqXeV"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "在这个 `FixedHiddenMLP` 模型中，我们实现了一个隐藏层，其权重（`self.rand_weight`）在实例化时被随机初始化，此后保持不变。这个权重不是一个模型参数，因此它永远不会被反向传播所更新。然后，网络将这个 \"固定 \"层的输出通过一个全连接层。"
      ],
      "metadata": {
        "id": "sRCfjOWr2WZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = FixedHiddenMLP()\n",
        "net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnoRQ7IO2tL0",
        "outputId": "5a38e5bc-a097-4ab5-bb08-34d7add3fcdd"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2917, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以混合和匹配各种方式将模块组装在一起:"
      ],
      "metadata": {
        "id": "s9FoiUMZ2ttW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NestMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
        "                                 nn.LazyLinear(32), nn.ReLU())\n",
        "        self.linear = nn.LazyLinear(16)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.linear(self.net(X))\n",
        "\n",
        "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
        "chimera(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QhUMmBC2u9B",
        "outputId": "78e90a55-e481-476c-dd9c-d6055baa9387"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1069, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.2. 参数管理（Parameter Management）**\n",
        "\n",
        "一旦我们选择了一个架构并设置了超参数，我们就进入训练循环，我们的目标是找到使损失函数最小的参数值。训练结束后，我们将需要这些参数，以便进行未来的预测。此外，我们有时希望提取这些参数，以便在其他情况下重新使用它们，将我们的模型保存在磁盘上，以便在其他软件中执行，或者用于检查。我们首先来具有一个隐藏层的 MLP 例子。"
      ],
      "metadata": {
        "id": "1kh1VtA43Kdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
        "X = torch.rand(size=(2, 4))\n",
        "net(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lca2QdsX27mU",
        "outputId": "e3c08ce3-0850-4d6d-e805-5081691cfab2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.2.1. 参数访问（Parameter Access）**\n",
        "\n",
        "当一个模型通过 `Sequential` 类定义时，我们可以首先通过对模型的索引来访问任何一层，就好像它是一个列表。每个层的参数都很方便地位于其属性中。我们可以按以下方式检查第二个全连接层的参数："
      ],
      "metadata": {
        "id": "AvcvmC2z30rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net[2].state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N02q26Lh3zEO",
        "outputId": "5bec83b1-5806-4fe5-b108-d7d66f5dcca1"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[ 0.3048,  0.2375, -0.0359,  0.0241,  0.3432, -0.2413,  0.3467, -0.1585]])),\n",
              "             ('bias', tensor([-0.3508]))])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.2.1.1. 目标参数（Targeted Parameters）**\n",
        "\n",
        "请注意，每个参数都被表示为 `parameter` 类的一个实例。要使用参数，我们首先需要访问底层的数值。有几种方法可以做到这一点。下面的代码从第二个神经网络层中提取偏差，返回一个参数类实例，并进一步访问该参数的值。"
      ],
      "metadata": {
        "id": "jY__TQg-461b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(net[2].bias), net[2].bias.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL7N7W-R4JYB",
        "outputId": "291b8c3d-6c51-4689-bd24-029c04928568"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.nn.parameter.Parameter, tensor([-0.3508]))"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Parameter` 是复杂的对象，包含值、梯度和附加信息。这就是为什么我们需要明确地请求该值。除了值之外，每个参数还允许我们访问梯度。因为我们还没有为这个网络调用反向传播，它处于初始状态。"
      ],
      "metadata": {
        "id": "vyPchmcp5tBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net[2].weight.grad == None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqgQ-Bdy548u",
        "outputId": "ffabcd59-2919-4071-a3dc-df841d372450"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.2.1.2. 访问所有参数（All Parameters at Once）**\n",
        "\n",
        "对于 `Sequential` 实例中含模型参数的层，我们可以通过 `Module` 类的 `parameters()` 或者 `named_parameters` 方法来访问所有参数（以迭代器的形式返回），后者除了返回参数 Tensor 外还会返回其名字。"
      ],
      "metadata": {
        "id": "fWBrDEao42fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[(name, param.shape) for name, param in net.named_parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY9cI3Nq43BR",
        "outputId": "72fa2b84-7dfd-4951-9334-4cc8ab40cbe1"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('0.weight', torch.Size([8, 4])),\n",
              " ('0.bias', torch.Size([8])),\n",
              " ('2.weight', torch.Size([1, 8])),\n",
              " ('2.bias', torch.Size([1]))]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.2.2. 捆绑参数（Tied Parameters）**\n",
        "\n",
        "通常情况下，我们想在多个层之间共享参数。下面，我们分配了一个全连接层，然后专门使用它的参数来设置另一个层的参数。这里我们需要在访问参数之前运行前向传播 `net(X)`。"
      ],
      "metadata": {
        "id": "6IUfhtR767tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to give the shared layer a name so that we can refer to its\n",
        "# parameters\n",
        "shared = nn.LazyLinear(8)\n",
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    nn.LazyLinear(1))\n",
        "net(X)\n",
        "# Check whether the parameters are the same\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
        "net[2].weight.data[0, 0] = 100\n",
        "# Make sure that they are actually the same object rather than just having the\n",
        "# same value\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq7sPTQP6aBV",
        "outputId": "6c7c31b6-cc63-4886-a865-fa9d99c35f3e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True, True, True, True, True, True, True])\n",
            "tensor([True, True, True, True, True, True, True, True])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.3. 参数初始化（Parameter Initialization）**\n",
        "\n",
        "默认情况下，PyTorch 会从一个根据输入和输出维度计算出来的范围中抽取，统一初始化权重和偏置矩阵。PyTorch 的 `nn.init` 模块提供了多种预设的初始化方法。"
      ],
      "metadata": {
        "id": "tSggTaURgwpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
        "X = torch.rand(size=(2, 4))\n",
        "net(X).shape"
      ],
      "metadata": {
        "id": "i5peJd1z7TGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889aefd1-1521-4650-84f9-ab2e2a1198f5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.3.1. 内置初始化（Built-in Initialization）**\n",
        "\n",
        "下面的代码将所有权重参数初始化为标准偏差为0.01的高斯随机变量，而偏置参数则清除为零。"
      ],
      "metadata": {
        "id": "2LlQqbyyhQ4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_normal(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
        "        nn.init.zeros_(module.bias)\n",
        "net.apply(init_normal)\n",
        "net[0].weight.data[0], net[0].bias.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pflbATzXhHJL",
        "outputId": "0b3533ed-36e5-433b-8445-3a3c8a1fcb99"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.0064,  0.0168, -0.0200, -0.0079]), tensor(0.))"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们也可以将所有的参数初始化为一个给定的常量值（比如，1）。"
      ],
      "metadata": {
        "id": "-JCkhuoqhi8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_constant(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.zeros_(module.bias)\n",
        "net.apply(init_constant)\n",
        "net[0].weight.data[0], net[0].bias.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfj_bHzDhafc",
        "outputId": "f4d29d37-8d23-49ef-8cef-0608651eae75"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1., 1.]), tensor(0.))"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们还可以为某些区块应用不同的初始化器。例如，下面我们用 Xavier 初始化器初始化第一层，并将第二层初始化为42的常量值。"
      ],
      "metadata": {
        "id": "83Kln3qnhr6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_xavier(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "def init_42(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        nn.init.constant_(module.weight, 42)\n",
        "\n",
        "net[0].apply(init_xavier)\n",
        "net[2].apply(init_42)\n",
        "print(net[0].weight.data[0])\n",
        "print(net[2].weight.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7h9A_EPhlB7",
        "outputId": "507df633-529e-42cb-fa3a-76ff8d777438"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.5611, -0.0304,  0.2399,  0.1844])\n",
            "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.3.2. 自定义初始化（Custom Initialization）**\n",
        "\n",
        "有时，我们需要的初始化方法并不是由深度学习框架提供的。在下面的例子中，我们使用以下奇怪的分布为任何权重参数 $w$ 定义一个初始化器。\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    w \\sim \\begin{cases}\n",
        "        U(5, 10) & \\text{ with probability } \\frac{1}{4} \\\\\n",
        "            0    & \\text{ with probability } \\frac{1}{2} \\\\\n",
        "        U(-10, -5) & \\text{ with probability } \\frac{1}{4}\n",
        "    \\end{cases}\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "PSlTElrWh7pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_init(module):\n",
        "    if type(module) == nn.Linear:\n",
        "        print(\"Init\", *[(name, param.shape)\n",
        "                        for name, param in module.named_parameters()][0])\n",
        "        nn.init.uniform_(module.weight, -10, 10)\n",
        "        module.weight.data *= module.weight.data.abs() >= 5\n",
        "\n",
        "net.apply(my_init)\n",
        "net[0].weight[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXxkqq02hwFv",
        "outputId": "0f32a846-7dc8-47cf-fb7a-33607a385b81"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init weight torch.Size([8, 4])\n",
            "Init weight torch.Size([1, 8])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-9.2129,  0.0000,  9.7765, -5.3081],\n",
              "        [ 6.7363,  9.6402, -0.0000,  6.7002]], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "请注意，我们总是可以选择直接设置参数:"
      ],
      "metadata": {
        "id": "l366Bu3Tikcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net[0].weight.data[:] += 1\n",
        "net[0].weight.data[0, 0] = 42\n",
        "net[0].weight.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNyPST1siedV",
        "outputId": "8e6a8c6b-1d93-4dad-9dd4-4fdf413617d3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([42.0000,  1.0000, 10.7765, -4.3081])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.5. 自定义层（Custom Layers）**\n",
        "\n",
        "深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层和后面章节中将要介绍的卷积层、池化层与循环层。虽然 PyTorch 提供了大量常用的层，但有时候我们依然希望自定义层。"
      ],
      "metadata": {
        "id": "Nn0xwbowjMT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.5.1. 不含模型参数的自定义层（Layers without Parameters）**\n",
        "\n",
        "下面的 `CenteredLayer` 类通过继承 `Module` 类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了 `forward` 函数里。这个层里不含模型参数。"
      ],
      "metadata": {
        "id": "VwbXNt1gjclA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class CenteredLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X - X.mean()"
      ],
      "metadata": {
        "id": "CuRKkGyqimYq"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以实例化这个层，然后做前向计算。"
      ],
      "metadata": {
        "id": "Rf6_pLcvkL0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = CenteredLayer()\n",
        "layer(torch.tensor([1.0, 2, 3, 4, 5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN1MX0Oyj9-l",
        "outputId": "6fc17943-4d65-42c4-88d1-93a08576f37a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2., -1.,  0.,  1.,  2.])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们也可以用它来构造更复杂的模型："
      ],
      "metadata": {
        "id": "AxTH75fEkQ6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())"
      ],
      "metadata": {
        "id": "qj0-KydukEmW"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = net(torch.rand(4, 8))\n",
        "Y.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdUxLaT0kGdN",
        "outputId": "eafb4b8f-a86a-4cc0-f8b6-9075dd8cf02b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.5879e-09, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.5.2. 含模型参数的自定义层（Layers with Parameters）**\n",
        "\n",
        "我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。我们可以使用内置函数来创建参数，它们提供一些基本的管理功能。现在让我们来实现我们自己版本的全连接层。这个层需要两个参数，一个表示权重，另一个表示偏置。在这个实现中，我们把 ReLU 激活作为一个默认值。该层需要两个输入参数：`in_units` 和 `units`，它们分别表示输入和输出的数量。"
      ],
      "metadata": {
        "id": "iTYA6bhPkW7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLinear(nn.Module):\n",
        "    def __init__(self, in_units, units):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
        "        self.bias = nn.Parameter(torch.randn(units,))\n",
        "\n",
        "    def forward(self, X):\n",
        "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
        "        return F.relu(linear)"
      ],
      "metadata": {
        "id": "7ayUBCwTkIB0"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear = MyLinear(5, 3)\n",
        "linear.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vULd4TW6lRuC",
        "outputId": "6de67b94-0203-4e01-910c-c30bbb919b9a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.8916,  1.1602, -1.0500],\n",
              "        [-1.2361,  0.8882, -0.2208],\n",
              "        [ 0.2404,  0.4173, -1.4291],\n",
              "        [ 0.0339,  1.4962, -0.5210],\n",
              "        [ 0.3537, -0.1940, -0.8185]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以直接使用自定义层进行前向传播计算。"
      ],
      "metadata": {
        "id": "gnVfnsIKlZIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear(torch.rand(2, 5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0-x7g8dlSGm",
        "outputId": "41aca251-221b-49db-e86d-b659ed125885"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 1.0567, 0.0000],\n",
              "        [0.0000, 1.8363, 0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们还可以使用自定义层来构建模型。一旦我们有了这个，我们就可以像内置的完全连接层一样使用它。"
      ],
      "metadata": {
        "id": "mKkSDd7yleGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
        "net(torch.rand(2, 64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QfRgAP_la62",
        "outputId": "27bea3bb-72ab-44d7-be9b-4f70655961cb"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[17.3037],\n",
              "        [10.7841]])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.6. 读取和存储（File I/O）**\n",
        "\n",
        "在实际中，我们有时需要把训练好的模型部署到很多不同的设备。在这种情况下，我们可以把内存中训练好的模型参数存储在硬盘上供后续读取使用。"
      ],
      "metadata": {
        "id": "ckMhV5kMlnQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.6.1. 读写 `Tensor`（Loading and Saving Tensors）**\n",
        "\n",
        "我们可以直接使用 `save` 函数和 `load` 函数分别存储和读取 Tensor。`save` 使用 Python 的 pickle 实用程序将对象进行序列化，然后将序列化的对象保存到 disk，使用 `save` 可以保存各种对象,包括模型、张量和字典等。而 `load` 使用 pickle unpickle 工具将 pickle 的对象文件反序列化为内存。"
      ],
      "metadata": {
        "id": "G8xCaGnHl2pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "x = torch.arange(4)\n",
        "torch.save(x, 'x-file')"
      ],
      "metadata": {
        "id": "xNsiO6nrlfyL"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在我们可以把存储文件中的数据读回内存中。"
      ],
      "metadata": {
        "id": "de2L6-DxmjeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = torch.load('x-file')\n",
        "x2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hH5Wcg6mWxt",
        "outputId": "f599bea8-132d-4acd-e85d-22040d154065"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以存储一个 `Tensor` 列表并将其读回内存。"
      ],
      "metadata": {
        "id": "3okYhmsnmnzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.zeros(4)\n",
        "torch.save([x, y],'x-files')\n",
        "x2, y2 = torch.load('x-files')\n",
        "(x2, y2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYu-p4s-mZeA",
        "outputId": "603f5991-d533-4758-b621-f81bc3090100"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们甚至可以写和读一个字典，从字符串映射到 `Tensor`。当我们想读取或写入一个模型中的所有权重时，这很方便。"
      ],
      "metadata": {
        "id": "rUjJH1dXnBh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydict = {'x': x, 'y': y}\n",
        "torch.save(mydict, 'mydict')\n",
        "mydict2 = torch.load('mydict')\n",
        "mydict2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtixkuwJmdM3",
        "outputId": "921d4d90-35b5-4b7c-89c4-6a9b7047df23"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.6.2. 读写模型（Loading and Saving Model Parameters）**\n",
        "\n",
        "保存单个权重向量（或其他张量）是很有用的，但如果我们想保存（以后再加载）整个模型，就会变得非常乏力了。毕竟，我们可能有数以百计的参数组洒在整个模型中。出于这个原因，深度学习框架提供了内置功能来加载和保存整个网络。需要注意的一个重要细节是，这**保存的是模型参数而不是整个模型**。因此，为了恢复一个模型，我们需要在代码中生成架构，然后从磁盘加载参数。"
      ],
      "metadata": {
        "id": "LkckVqFwnIS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.LazyLinear(256)\n",
        "        self.output = nn.LazyLinear(10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.output(F.relu(self.hidden(x)))\n",
        "\n",
        "net = MLP()\n",
        "X = torch.randn(size=(2, 20))\n",
        "Y = net(X)"
      ],
      "metadata": {
        "id": "aPCEFnEeme2n"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们将模型的参数存储为一个名为 \"mlp.params \"的文件。"
      ],
      "metadata": {
        "id": "GDvCdqH1oOe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net.state_dict(), 'mlp.params')"
      ],
      "metadata": {
        "id": "g20ZENcInnzP"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了恢复模型，我们实例化了一个原始 MLP 模型的克隆版本。我们没有随机地初始化模型参数，而是直接读取存储在文件中的参数。"
      ],
      "metadata": {
        "id": "UJG4Xi6zoT6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clone = MLP()\n",
        "clone.load_state_dict(torch.load('mlp.params'))\n",
        "clone.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmC8QrpLoQeG",
        "outputId": "c8afd363-c5b3-4d22-ac7b-3749fbf199ee"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (hidden): LazyLinear(in_features=0, out_features=256, bias=True)\n",
              "  (output): LazyLinear(in_features=0, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "由于两个实例有相同的模型参数，相同的输入 $X$ 的计算结果应该是相同的。让我们来验证这一点。"
      ],
      "metadata": {
        "id": "N2UN_VUOobu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_clone = clone(X)\n",
        "Y_clone == Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIjrlBrcoYgQ",
        "outputId": "db896645-1452-4ca8-b206-0804f30dfd18"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
              "        [True, True, True, True, True, True, True, True, True, True]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.7. GPU计算（GPUs）**\n",
        "\n",
        "到目前为止，我们一直在使用CPU计算。对复杂的神经网络和大规模的数据来说，使用 CPU 来计算可能不够高效。在本节中，我们将介绍如何使用 NVIDIA GPU来计算。"
      ],
      "metadata": {
        "id": "9HcTPhgRoo9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zD74j0dofx-",
        "outputId": "89d8aeba-d84e-4820-f3ef-398fe5815542"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb  9 19:54:50 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    32W /  70W |    794MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      2563      C                                     791MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.7.1. 计算设备（Computing Devices）**\n",
        "\n",
        "我们可以指定设备，如 CPU 和 GPU，用于存储和计算。默认情况下，`Tensor` 是在主内存中创建的，然后使用 CPU 来计算。"
      ],
      "metadata": {
        "id": "Eby09Pt-qeqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "用 `torch.cuda.is_available()` 查看 GPU 是否可用:"
      ],
      "metadata": {
        "id": "WD0kOIpQq8Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAgTDeoNrBOq",
        "outputId": "abce6ee4-7b00-4246-fc46-e30572659065"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 PyTorch 中，CPU 和 GPU 可以通过 `torch.device('cpu')` 和 `torch.device('cuda')` 表示。需要注意的是，`cpu` 设备意味着所有的物理 CPU 和内存。这意味着 PyTorch 的计算将尝试使用所有的 CPU 核心。然而，一个 `gpu` 设备只代表一个卡和相应的内存。如果有多个 GPU，我们使用 `torch.device(f'cuda:{i}')` 来代表第 $i$ 个 GPU（从0开始）。"
      ],
      "metadata": {
        "id": "AEFVEdhWrVwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def cpu():\n",
        "    return torch.device('cpu')\n",
        "def gpu(i=0):\n",
        "    return torch.device(f'cuda:{i}')\n",
        "cpu(), gpu(), gpu(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0Yt4mRzo7n1",
        "outputId": "67da038c-81b7-4a7d-a0b7-e02d60abde3a"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cpu'),\n",
              " device(type='cuda', index=0),\n",
              " device(type='cuda', index=1))"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_gpus():\n",
        "    return torch.cuda.device_count()\n",
        "num_gpus()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TgAjOhipfKE",
        "outputId": "debbf52d-e8b7-4dec-a995-ad7db7cb7c45"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.7.2. `Tensor` 的 GPU 计算（Tensors and GPUs）**\n",
        "\n",
        "默认情况下，`Tensor` 是在 CPU 上创建的。我们可以查询 `Tensor` 所在的设备："
      ],
      "metadata": {
        "id": "ayIdMT_jr9Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "x.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSKZovs6pp0h",
        "outputId": "2704abf2-d35c-4393-8a06-602d9065a3c8"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "值得注意的是，只要我们想对多个项进行操作，它们就必须在同一个设备上。例如，如果我们对两个 `Tensor` 求和，我们需要**确保两个参数都在同一个设备上**--否则，框架将不知道在哪里存储结果，甚至不知道如何决定在哪里进行计算。"
      ],
      "metadata": {
        "id": "uMQV0RJ_sSJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用 `.cuda()` 可以将 CPU 上的 `Tensor` 转换（复制）到 GPU 上。"
      ],
      "metadata": {
        "id": "Q_80yh2ns7xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.cuda(0)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGhEdGPfsQDR",
        "outputId": "682859e1-b2b2-49bc-ab59-b99f6c724e1f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以通过 `Tensor` 的 `device` 属性来查看该 `Tensor` 所在的设备。"
      ],
      "metadata": {
        "id": "OT5gBV9QtrH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrMatBLztm-g",
        "outputId": "2b919c47-c844-42b1-ef1b-f3b37ac0efc6"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以直接在创建的时候就指定设备："
      ],
      "metadata": {
        "id": "yiRmR6iMt2nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "x = torch.tensor([1, 2, 3], device=device)\n",
        "# or\n",
        "x = torch.tensor([1, 2, 3]).to(device)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8PdHepNtxlO",
        "outputId": "fe28a1e2-467a-4291-e29c-1406e9fe3d5b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6.7.3. 神经网络的GPU计算（Neural Networks and GPUs）**\n",
        "\n",
        "同 `Tensor` 类似，PyTorch 模型也可以通过 `.cuda` 转换到 GPU 上。我们可以通过检查模型的参数的 `device` 属性来查看存放模型的设备。"
      ],
      "metadata": {
        "id": "qPrF1TfOuNWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = nn.Sequential(nn.LazyLinear(1))\n",
        "net.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtTIbFsxt-hA",
        "outputId": "d878b336-885e-4eaf-bd39-8c6108bfb786"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): LazyLinear(in_features=0, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.ones(2, 3, device=device)\n",
        "net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBSf9sE1u21F",
        "outputId": "e3cf8ba6-86b6-48ea-f1e1-3f51817a5aca"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9156],\n",
              "        [0.9156]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net[0].weight.data.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGVHpXpdu9Jm",
        "outputId": "8d48f544-0b92-4a4b-ff69-49c0c36c4a05"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html 06_D2L_Builders_Guide.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVIsMGpavjgQ",
        "outputId": "6f8bab42-a961-434a-b906-b2d8d45c451e"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook 06_D2L_Builders_Guide.ipynb to html\n",
            "[NbConvertApp] Writing 371583 bytes to 06_D2L_Builders_Guide.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    }
  ]
}